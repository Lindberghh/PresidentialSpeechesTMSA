{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Sentiment Analysis"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7033a9aad190718"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-04T22:25:00.324304500Z",
     "start_time": "2023-10-04T22:24:57.963298400Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from gensim.matutils import corpus2csc\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import html\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "data = pd.read_json('venv/data/presidential_speeches.json') # load data\n",
    "data.sort_values(by=['date'], inplace=True) # sort by data\n",
    "data.reset_index(drop=True, inplace=True) # reset index"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-04T22:25:00.468301400Z",
     "start_time": "2023-10-04T22:25:00.319300800Z"
    }
   },
   "id": "2873e50ca953d121"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# sort presidents to their parties\n",
    "president_list = []\n",
    "for i in range(len(data['president'].unique())):\n",
    "    president_list.append(data['president'].unique()[i])\n",
    "parties_list= ['Federalist', 'Democratic-Republican', 'National Republican', 'Democratic', 'Whig', 'Republican', 'Democratic (Union)']\n",
    "sort_party = [0, 0, 1, 1, 1, 2, 3, 3, 4, 4, 3, 4, 4, 5, 3, 3, 6, 5, 5, 5, 5, 3, 5, 5, 5, 5, 3, 5, 5, 5, 3, 3, 5, 5, 3, 3, 5, 5, 3, 5, 3, 5, 3, 5, 3]\n",
    "party = []\n",
    "for i in range(len(sort_party)):\n",
    "    party.append(parties_list[sort_party[i]])\n",
    "data['party'] = pd.Series(dtype='string')\n",
    "for i in range(len(president_list)):\n",
    "    data['party'][data['president'] == president_list[i]] = party[i]\n",
    "# assign each speech its respective era\n",
    "data['era'] = pd.Series(dtype='string')\n",
    "era_list = ['Early Republic', 'Jacksonian Democracy', 'Sectional Conflict', 'Gilded Age', 'Progressive Era', 'Depression and World Conflict', 'Social Change and Soviet Relations', 'Globalization']\n",
    "for i in range(len(data)):\n",
    "    if data['date'][i] < pd.Timestamp('1829-01-01T12'):\n",
    "        data['era'][i] =  era_list[0]\n",
    "    if pd.Timestamp('1829-01-01T12') <= data['date'][i] < pd.Timestamp('1853-01-01T12'):\n",
    "        data['era'][i] = era_list[1]\n",
    "    elif pd.Timestamp('1853-01-01T12') <= data['date'][i] < pd.Timestamp('1881-01-01T12'):\n",
    "        data['era'][i] = era_list[2]\n",
    "    elif pd.Timestamp('1881-01-01T12') <= data['date'][i] < pd.Timestamp('1897-01-01T12'):\n",
    "        data['era'][i] = era_list[3]\n",
    "    elif pd.Timestamp('1897-01-01T12') <= data['date'][i] < pd.Timestamp('1921-01-01T12'):\n",
    "        data['era'][i] = era_list[4]\n",
    "    elif pd.Timestamp('1921-01-01T12') <= data['date'][i] < pd.Timestamp('1961-01-01T12'):\n",
    "        data['era'][i] = era_list[5]\n",
    "    elif pd.Timestamp('1961-01-01T12') <= data['date'][i] < pd.Timestamp('1989-01-01T12'):\n",
    "        data['era'][i] = era_list[6]\n",
    "    elif pd.Timestamp('1989-01-01T12') <= data['date'][i]:\n",
    "        data['era'][i] = era_list[7]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-04T22:25:01.331297200Z",
     "start_time": "2023-10-04T22:25:01.119300400Z"
    }
   },
   "id": "13352c4c7ecf9694"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "845b4a47aa126ce1"
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Enno\\AppData\\Local\\Temp/ipykernel_21340/192661657.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['transcript'][i] = data['transcript'][i].replace('\\'', '')\n"
     ]
    }
   ],
   "source": [
    "data['transcript'].replace(to_replace='(Applause.)', regex=True, value='', inplace=True)\n",
    "data['transcript'].replace(to_replace='(Laughter.)', regex=True, value='', inplace=True)\n",
    "for i in range(len(data)):\n",
    "    data['transcript'][i] = data['transcript'][i].replace('\\'', '')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-04T23:19:59.973173400Z",
     "start_time": "2023-10-04T23:19:59.783179900Z"
    }
   },
   "id": "6876427c46d93147"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import contractions\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from textblob import TextBlob\n",
    "from unidecode import unidecode"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-04T23:20:03.540707800Z",
     "start_time": "2023-10-04T23:20:03.529704500Z"
    }
   },
   "id": "5be2781321071fe4"
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "def lemmatize_pos_tagged_text(text, lemmatizer, pos_tag_dict):\n",
    "  sentences = nltk.sent_tokenize(text)\n",
    "  new_sentences = []\n",
    "\n",
    "  for sentence in sentences:\n",
    "    sentence = sentence.lower()\n",
    "    new_sentence_words = []\n",
    "    #one pos_tuple for sentence\n",
    "    pos_tuples = nltk.pos_tag(nltk.word_tokenize(sentence)) \n",
    "\n",
    "    for word_idx, word in enumerate(nltk.word_tokenize(sentence)):\n",
    "      nltk_word_pos = pos_tuples[word_idx][1]\n",
    "      wordnet_word_pos = pos_tag_dict.get(\n",
    "                          nltk_word_pos[0].upper(), None)\n",
    "      if wordnet_word_pos is not None:\n",
    "        new_word = lemmatizer.lemmatize(word, wordnet_word_pos)\n",
    "      else:\n",
    "        new_word = lemmatizer.lemmatize(word)\n",
    "\n",
    "      new_sentence_words.append(new_word)\n",
    "\n",
    "    new_sentence = \" \".join(new_sentence_words)\n",
    "    new_sentences.append(new_sentence)\n",
    "\n",
    "  return \" \".join(new_sentences)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-04T23:20:04.898700100Z",
     "start_time": "2023-10-04T23:20:04.881702300Z"
    }
   },
   "id": "f80fdd2094b9b3ba"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5372793e26be34"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "def download_if_non_existent(res_path, res_name):\n",
    "  try:\n",
    "    nltk.data.find(res_path)\n",
    "  except LookupError:\n",
    "    print(f'resource {res_path} not found. Downloading now...')\n",
    "    nltk.download(res_name)\n",
    "class NltkPreprocessingSteps:\n",
    "  def __init__(self, X):\n",
    "    self.X = X\n",
    "    download_if_non_existent('corpora/stopwords', 'stopwords')\n",
    "    download_if_non_existent('tokenizers/punkt', 'punkt')\n",
    "    download_if_non_existent('taggers/averaged_perceptron_tagger',\n",
    "                             'averaged_perceptron_tagger')\n",
    "    download_if_non_existent('corpora/wordnet', 'wordnet')\n",
    "    download_if_non_existent('corpora/omw-1.4', 'omw-1.4')\n",
    "\n",
    "    self.sw_nltk = stopwords.words('english')\n",
    "    new_stopwords = ['<*>']\n",
    "    self.sw_nltk.extend(new_stopwords)\n",
    "    self.sw_nltk.remove('not')\n",
    "\n",
    "    self.pos_tag_dict = {\"J\": wordnet.ADJ,\n",
    "                    \"N\": wordnet.NOUN,\n",
    "                    \"V\": wordnet.VERB,\n",
    "                    \"R\": wordnet.ADV}\n",
    "\n",
    "    # '!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~' 32 punctuations in python\n",
    "    # we dont want to replace . first time around\n",
    "    self.remove_punctuations = string.punctuation.replace('.','')\n",
    "\n",
    "  def remove_html_tags(self):\n",
    "    self.X = self.X.apply(\n",
    "            lambda x: BeautifulSoup(x, 'html.parser').get_text())\n",
    "    return self\n",
    "\n",
    "  def replace_diacritics(self):\n",
    "    self.X = self.X.apply(\n",
    "            lambda x: unidecode(x, errors=\"preserve\"))\n",
    "    return self\n",
    "\n",
    "  def to_lower(self):\n",
    "    self.X = np.apply_along_axis(lambda x: x.lower(), self.X)\n",
    "    return self\n",
    "\n",
    "  def expand_contractions(self):\n",
    "    self.X = self.X.apply(\n",
    "            lambda x: \" \".join([contractions.fix(expanded_word) \n",
    "                        for expanded_word in x.split()]))\n",
    "    return self\n",
    "\n",
    "  def remove_numbers(self):\n",
    "    self.X = self.X.apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "    return self\n",
    "\n",
    "  def replace_dots_with_spaces(self):\n",
    "    self.X = self.X.apply(lambda x: re.sub(\"[.]\", \" \", x))\n",
    "    return self\n",
    "\n",
    "  def remove_punctuations_except_periods(self):\n",
    "    self.X = self.X.apply(\n",
    "                 lambda x: re.sub('[%s]' %\n",
    "                  re.escape(self.remove_punctuations), '' , x))\n",
    "    return self\n",
    "\n",
    "  def remove_all_punctuations(self):\n",
    "    self.X = self.X.apply(lambda x: re.sub('[%s]' %\n",
    "                          re.escape(string.punctuation), '' , x))\n",
    "    return self\n",
    "\n",
    "  def remove_double_spaces(self):\n",
    "    self.X = self.X.apply(lambda x: re.sub(' +', ' ', x))\n",
    "    return self\n",
    "\n",
    "  def fix_typos(self):\n",
    "    self.X = self.X.apply(lambda x: str(TextBlob(x).correct()))\n",
    "    return self\n",
    "\n",
    "  def remove_stopwords(self):\n",
    "    # remove stop words from token list in each column\n",
    "    self.X = self.X.apply(\n",
    "            lambda x: \" \".join([ word for word in x.split() \n",
    "                     if word not in self.sw_nltk]) )\n",
    "    return self\n",
    "\n",
    "  # lemmatize(self):\n",
    "    #lemmatizer = WordNetLemmatizer()\n",
    "    #self.X = self.X.apply(lambda x: lemmatize_pos_tagged_text(\n",
    "       #                    x, lemmatizer, self.pos_tag_dict))\n",
    "    #return self\n",
    "\n",
    "  def get_processed_text(self):\n",
    "    return self.X"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-04T23:20:06.367702600Z",
     "start_time": "2023-10-04T23:20:06.355705900Z"
    }
   },
   "id": "1752a7494125cfae"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resource corpora/wordnet not found. Downloading now...\n",
      "resource corpora/omw-1.4 not found. Downloading now...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Enno\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Enno\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "txt_preproc = NltkPreprocessingSteps(data['transcript'])\n",
    "processed_text = \\\n",
    "    txt_preproc \\\n",
    "    .remove_html_tags()\\\n",
    "    .replace_diacritics()\\\n",
    "    .expand_contractions()\\\n",
    "    .remove_numbers()\\\n",
    "    .fix_typos()\\\n",
    "    .remove_double_spaces()\\\n",
    "    .get_processed_text()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-10-04T23:20:25.721713700Z"
    }
   },
   "id": "a5979166bdb144dd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(len(processed_text)):\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53d0725ebd35281a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
